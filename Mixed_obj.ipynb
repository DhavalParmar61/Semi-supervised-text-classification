{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mixed_obj.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sI_JdWNOC22l"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Vn4KMxl5Xd5"},"source":["cd /content/drive/MyDrive/Colab Notebooks/DL_NLP/term_proj/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6X8s9qUiGMXQ"},"source":["#Install Requirement\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"Q2QhDGOeGL3V"},"source":["pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0S1SUVPEDbCe"},"source":["#Packages"]},{"cell_type":"code","metadata":{"id":"-Rw_SRiqDaRi"},"source":["import numpy as np\r\n","import torch \r\n","import json\r\n","import pickle\r\n","import os\r\n","import torch\r\n","import random\r\n","import errno\r\n","import collections\r\n","from collections import Counter\r\n","import tensorflow as tf\r\n","from sklearn.model_selection import train_test_split\r\n","from chainer.dataset import convert\r\n","from torch.optim import Adam,lr_scheduler,SGD,RMSprop\r\n","import nltk\r\n","nltk.download('stopwords')\r\n","nltk.download('wordnet')\r\n","nltk.download('punkt')\r\n","from nltk.tokenize import word_tokenize\r\n","from nltk.stem import PorterStemmer\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer\r\n","from torchtext import data\r\n","from __future__ import division\r\n","from torch.autograd import Variable\r\n","import torch.nn as nn\r\n","import torch.nn.init as weight_init\r\n","import torch.nn.functional as F\r\n","import sys\r\n","from tqdm import tqdm\r\n","import unicodedata\r\n","from tensorboardX import SummaryWriter\r\n","from collections import defaultdict\r\n","from sklearn.utils import compute_class_weight\r\n","import math\r\n","import string\r\n","import io\r\n","import six\r\n","import pprint\r\n","from metrics import ConfusionMatrix\r\n","from argparse import ArgumentParser\r\n","from datetime import datetime\r\n","from collections import namedtuple\r\n","import gensim.downloader\r\n","import logging\r\n","import time\r\n","import itertools\r\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLy1HVBeF8Kg"},"source":["device=torch.device(\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2FpBz86HDepn"},"source":["#Load DATA and Embedding"]},{"cell_type":"code","metadata":{"id":"sExrtcm6DksN"},"source":["#Preprocess Data\r\n","def get_data(file_name,max_nv):\r\n","    f = open(file_name,'r')\r\n","    count = Counter()\r\n","    labels = []\r\n","    texts = []\r\n","    stop_words = set(stopwords.words('english'))\r\n","    lemmatizer = WordNetLemmatizer()\r\n","    for line in f:\r\n","      line = line.split(\"\\t\")\r\n","      labels.append(int(line[0]))\r\n","      text = line[1]\r\n","      text = text.lower()\r\n","      text = text.replace('quot','')\r\n","      text = ''.join([i for i in text if not i.isdigit()])\r\n","      tokens = word_tokenize(text)\r\n","      temp = [i for i in tokens if not i in stop_words]\r\n","      if (len(temp)!=0):\r\n","        text = temp\r\n","      text = [lemmatizer.lemmatize(w) for w in text]\r\n","      for word in text:\r\n","        count[word] += 1\r\n","      texts.append(text)\r\n","    vocab = [word for (word, _) in count.most_common(max_nv)]\r\n","    f.close()\r\n","    return vocab,labels,texts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5nEMYnFPup6"},"source":["#Read Data and make vocab\r\n","train_vocab , train_label, train_texts = get_data('./data/aclImdb_tok/train.txt',80000)\r\n","test_vocab , test_label, test_texts = get_data('./data/aclImdb_tok/test.txt',80000)\r\n","unlabel_vocab , _, unlabel_texts = get_data('./data/aclImdb_tok/unlabel.txt',80000)\r\n","# train_vocab , train_label, train_texts = get_data('./data/Agnews/train.txt',75000)\r\n","# test_vocab , test_label, test_texts = get_data('./data/Agnews/test.txt',75000)\r\n","# unlabel_vocab , _, unlabel_texts = get_data('./data/Agnews/unlabel.txt',75000)\r\n","vocab = list(set(train_vocab + unlabel_vocab))\r\n","vocab = vocab + ['<pad>', '<eos>', '<unk>', '<bos>']\r\n","w2id = {word: index for index, word in enumerate(vocab)}\r\n","id2w = {i: w for w, i in w2id.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1n6nU2_Lwbv"},"source":["#Load saved wordvector matrix\r\n","wordvector = np.load('./data/demo.word_vectors.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mz5_-IRALBr-"},"source":["#Load saved id to word maping\r\n","id2w = pickle.load(open('id2w.pickle','rb'))\r\n","w2id = {w: i for i, w in id2w.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8B-G99bVz4p"},"source":["#Prepare data\r\n","Special_Seq = namedtuple('Special_Seq', ['PAD', 'EOS', 'UNK', 'BOS'])\r\n","Vocab_Pad = Special_Seq(PAD=0, EOS=1, UNK=2, BOS=3)\r\n","\r\n","def make_dataset(text, w2id):\r\n","    dataset = []\r\n","    for line in text:\r\n","        array = np.asarray([w2id.get(word, Vocab_Pad.UNK) for word in line])\r\n","        dataset.append(array)\r\n","    return dataset\r\n","\r\n","#Label training data\r\n","temp = make_dataset(train_texts, w2id)\r\n","train_data = [(l, s) for l, s in six.moves.zip(train_label, temp)]\r\n","train_data,dev_data = train_test_split(train_data,test_size=0.1)\r\n","\r\n","#Unlabel training data\r\n","temp = make_dataset(unlabel_texts, w2id)\r\n","unlabel_data = [(l, s) for l, s in six.moves.zip(_, temp)]\r\n","\r\n","#Test data\r\n","temp = make_dataset(test_texts, w2id)\r\n","test_data = [(l, s) for l, s in six.moves.zip(test_label, temp)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwqIuHgmfbd-"},"source":["#Download pretrained word embedding while training first time\r\n","embedding = gensim.downloader.load('word2vec-google-news-300')\r\n","# embedding = gensim.downloader.load('fasttext-wiki-news-subwords-300')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bu5lP4LInvBe"},"source":["#Prepare wordembedding matrix while training first time\r\n","wordvector = []\r\n","for i in range(len(id2w)):\r\n","    word = id2w[i]\r\n","    if word in embedding:\r\n","      wordvector.append(embedding[word])\r\n","    else:\r\n","      wordvector.append(np.random.uniform(-0.25, 0.25, 300))\r\n","\r\n","wordvector = np.array(wordvector,dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_h7lQpVl9jd"},"source":["#save id 2 word maping\r\n","with open('id2w.pickle', 'wb') as f:\r\n","    pickle.dump(id2w, f, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7HQSPA-40GLx"},"source":["#save wordvector matrix\r\n"," np.save(os.path.join('data', 'demo' + '.word_vectors.npy'), wordvector)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gdwd2pBI9mUG"},"source":["#Definations"]},{"cell_type":"code","metadata":{"id":"mJk4z-T39kAb"},"source":["FLOAT_TYPE = torch.cuda.FloatTensor\r\n","INT_TYPE = torch.cuda.IntTensor\r\n","LONG_TYPE = torch.cuda.LongTensor\r\n","BYTE_TYPE = torch.cuda.ByteTensor\r\n","\r\n","Batch = collections.namedtuple('Batch', ['batch_size','labels','word_ids','sent_len'])\r\n","\r\n","def ensure_directory(directory):\r\n","    directory = os.path.expanduser(directory)\r\n","    try:\r\n","        os.makedirs(directory)\r\n","    except OSError as e:\r\n","        if e.errno != errno.EEXIST:\r\n","            raise e\r\n","\r\n","\r\n","def batch_size_fn(new, count, sofar):\r\n","    if count == 1:\r\n","        max_src_in_batch = 0\r\n","    max_src_in_batch = max(max_src_in_batch, len(new[1]) + 2)\r\n","    src_elements = count * max_src_in_batch\r\n","    return src_elements\r\n","\r\n","def get_accuracy(cm, output, target):\r\n","    batch_size = output.size(0)\r\n","    predictions = output.max(-1)[1].type_as(target)\r\n","    correct = predictions.eq(target)\r\n","    correct = correct.float()\r\n","    if not hasattr(correct, 'sum'):\r\n","        correct = correct.cpu()\r\n","    correct = correct.sum()\r\n","    cm.add_batch(target.cpu().numpy(), predictions.cpu().numpy())\r\n","    return correct\r\n","\r\n","def _get_trainabe_modules():\r\n","    param_list = list(embedder.parameters()) + list(encoder.parameters()) + list(clf.parameters())\r\n","    if lambda_ae > 0:\r\n","        param_list += list(ae.parameters())\r\n","    return param_list\r\n","\r\n","def at_loss(embedder, encoder, clf, batch, perturb_norm_length=5.0):\r\n","    embedded = embedder(batch)\r\n","    embedded.retain_grad()\r\n","    ce = F.cross_entropy((clf(encoder(embedded, batch)[0])), batch.labels)\r\n","    ce.backward()\r\n","\r\n","    d = embedded.grad.data.transpose(0, 1).contiguous()\r\n","    d = get_normalized_vector(d)\r\n","    d = d.transpose(0, 1).contiguous()\r\n","\r\n","    d = embedder(batch) + (perturb_norm_length * Variable(d))\r\n","    loss = F.cross_entropy(clf(encoder(d, batch)[0]), batch.labels)\r\n","    return loss\r\n","\r\n","\r\n","def get_normalized_vector(d):\r\n","    B, T, D = d.shape\r\n","    d = d.view(B, -1)\r\n","    d /= (1e-12 + torch.max(torch.abs(d), dim=1, keepdim=True)[0])\r\n","\r\n","    d /= torch.sqrt(1e-6 + torch.sum(d**2, dim=1, keepdim=True))\r\n","    d = d.view(B, T, D)\r\n","    return d\r\n","\r\n","def rnn_factory(rnn_type, **kwargs):\r\n","  \r\n","    no_pack_padded_seq = False\r\n","    rnn = getattr(nn, rnn_type)(**kwargs)\r\n","    return rnn, no_pack_padded_seq\r\n","\r\n","\r\n","def kl_categorical(p_logit, q_logit):\r\n","    p = F.softmax(p_logit, dim=-1)\r\n","    _kl = torch.sum(p * (F.log_softmax(p_logit, dim=-1) -\r\n","                         F.log_softmax(q_logit, dim=-1)), 1)\r\n","    return torch.mean(_kl) # F.sum(_kl) / xp.prod(xp.array(_kl.shape))\r\n","\r\n","\r\n","def vat_loss(embedder, encoder, clf, batch, perturb_norm_length=5.0,\r\n","             small_constant_for_finite_diff=1e-1, Ip=1, p_logit=None):\r\n","    embedded = embedder(batch)\r\n","    d = torch.randn(embedded.shape).type(FLOAT_TYPE)\r\n","    d = d.transpose(0, 1).contiguous()\r\n","    d = get_normalized_vector(d).transpose(0, 1).contiguous()\r\n","    for ip in range(Ip):\r\n","        x_d = Variable(embedded.data + (small_constant_for_finite_diff * d), requires_grad=True)\r\n","        x_d.retain_grad()\r\n","        p_d_logit = clf(encoder(x_d, batch)[0])\r\n","        kl_loss = kl_categorical(Variable(p_logit.data), p_d_logit)\r\n","        kl_loss.backward()\r\n","        d = x_d.grad.data.transpose(0, 1).contiguous()\r\n","        d = get_normalized_vector(d).transpose(0, 1).contiguous()\r\n","    x_adv = embedded + (perturb_norm_length * Variable(d))\r\n","    p_adv_logit = clf(encoder(x_adv, batch)[0])\r\n","    return kl_categorical(Variable(p_logit.data), p_adv_logit)\r\n","\r\n","\r\n","def entropy_loss(p_logit):\r\n","    p = F.softmax(p_logit, dim=-1)\r\n","    return -1 * torch.sum(p * F.log_softmax(p_logit, dim=-1)) / p_logit.size()[0]\r\n","\r\n","\r\n","def seq_func(func, x, reconstruct_shape=True, pad_remover=None):\r\n","    batch, length, units = x.shape\r\n","    e = x.view(batch * length, units)\r\n","    if pad_remover:\r\n","        e = pad_remover.remove(e)\r\n","    e = func(e)\r\n","    if pad_remover:\r\n","        e = pad_remover.restore(e)\r\n","    if not reconstruct_shape:\r\n","        return e\r\n","    out_units = e.shape[1]\r\n","    e = e.view(batch, length, out_units)\r\n","    assert (e.shape == (batch, length, out_units))\r\n","    return e\r\n","\r\n","def _linear(in_sz, out_sz, unif):\r\n","    l = nn.Linear(in_sz, out_sz)\r\n","    weight_init.xavier_uniform(l.weight.data)\r\n","    return l\r\n","\r\n","\r\n","def _append2seq(seq, modules):\r\n","    for module_ in modules:\r\n","        seq.add_module(str(module_), module_)\r\n","\r\n","\r\n","def binary_cross_entropy(x, y, smoothing=0., epsilon=1e-12):\r\n","    y = y.float()\r\n","    if smoothing > 0:\r\n","        smoothing *= 2\r\n","        y = y * (1 - smoothing) + 0.5 * smoothing\r\n","    return -torch.mean(\r\n","        torch.log(x + epsilon) * y + torch.log(1.0 - x + epsilon) * (1 - y))\r\n","    \r\n","   \r\n","\r\n","def aeq(*args):\r\n","    arguments = (arg for arg in args)\r\n","    first = next(arguments)\r\n","    assert all(arg == first for arg in arguments), \\\r\n","        \"Not all arguments have the same value: \" + str(args)\r\n","\r\n","\r\n","def sequence_mask(lengths, max_len=None):\r\n","    batch_size = lengths.numel()\r\n","    max_len = max_len or lengths.max()\r\n","    return (torch.arange(0, max_len)\r\n","            .type_as(lengths)\r\n","            .repeat(batch_size, 1)\r\n","            .lt(lengths.unsqueeze(1)))\r\n","    \r\n","\r\n","def embedded_dropout(embed, words, dropout=0.1, scale=None):\r\n","    if dropout:\r\n","        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\r\n","        mask = torch.autograd.Variable(mask)\r\n","        masked_embed_weight = mask * embed.weight\r\n","    else:\r\n","        masked_embed_weight = embed.weight\r\n","    if scale:\r\n","        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\r\n","\r\n","    padding_idx = embed.padding_idx\r\n","    if padding_idx is None:\r\n","        padding_idx = -1\r\n","\r\n","    X = F.embedding(words,masked_embed_weight,padding_idx,embed.max_norm,\r\n","                    embed.norm_type,embed.scale_grad_by_freq,embed.sparse)\r\n","    return X\r\n","\r\n","\r\n","def embedded_adaptive_dropout(embed, words, dropout=0.1, scale=None, is_training=True):\r\n","    if is_training:\r\n","        mask = embed.weight.data.new().resize_(\r\n","            (embed.weight.size(0), 1)).bernoulli_(1 - dropout)\r\n","        mask = (mask / torch.unsqueeze((1 - dropout), -1)).expand_as(\r\n","            embed.weight)\r\n","        mask = torch.autograd.Variable(mask)\r\n","        masked_embed_weight = mask * embed.weight\r\n","    else:\r\n","        masked_embed_weight = embed.weight\r\n","    if scale:\r\n","        masked_embed_weight = scale.expand_as(\r\n","            masked_embed_weight) * masked_embed_weight\r\n","\r\n","    padding_idx = embed.padding_idx\r\n","    if padding_idx is None:\r\n","        padding_idx = -1\r\n","\r\n","    X = embed._backend.Embedding.apply(words,masked_embed_weight,padding_idx,\r\n","                                       embed.max_norm,embed.norm_type,embed.scale_grad_by_freq,embed.sparse)\r\n","    return X\r\n","\r\n","def seq_pad_concat(batch, device):\r\n","    labels, word_ids = zip(*batch)\r\n","\r\n","    block_w = convert.concat_examples(word_ids,device,padding=Vocab_Pad.PAD)\r\n","\r\n","    sent_len = np.array(list(map(lambda x: len(x), word_ids)))\r\n","    block_w = Variable(torch.LongTensor(block_w).type(LONG_TYPE),\r\n","                       requires_grad=False)\r\n","    labels = Variable(torch.LongTensor(labels).type(LONG_TYPE),\r\n","                      requires_grad=False)\r\n","\r\n","    return Batch(batch_size=len(labels),word_ids=block_w.transpose(0, 1).contiguous(),labels=labels,sent_len=sent_len)\r\n","\r\n","\r\n","def seq2seq_pad_concat(ly_batch,device,eos_id=Vocab_Pad.EOS, bos_id=Vocab_Pad.BOS):\r\n","    labels, y_seqs = zip(*ly_batch)\r\n","    y_block = convert.concat_examples(y_seqs, device, padding=0)\r\n","\r\n","    y_out_block = np.pad(y_block, ((0, 0), (0, 1)), 'constant', constant_values=0)\r\n","    for i_batch, seq in enumerate(y_seqs):\r\n","        y_out_block[i_batch, len(seq)] = eos_id\r\n","    y_in_block = np.pad(y_block, ((0, 0), (1, 0)), 'constant', constant_values=bos_id)\r\n","\r\n","    y_in_block = Variable(torch.LongTensor(y_in_block).type(LONG_TYPE),\r\n","                          requires_grad=False)\r\n","    y_out_block = Variable(torch.LongTensor(y_out_block).type(LONG_TYPE),\r\n","                           requires_grad=False)\r\n","    return y_in_block, y_out_block\r\n","\r\n","def long_0_tensor_alloc(nelements, dtype=None):\r\n","    lt = long_tensor_alloc(nelements)\r\n","    lt.zero_()\r\n","    return lt\r\n","\r\n","\r\n","def long_tensor_alloc(dims, dtype=None):\r\n","    if type(dims) == int or len(dims) == 1:\r\n","        return torch.LongTensor(dims)\r\n","    return torch.LongTensor(*dims)\r\n","\r\n","\r\n","def print_sentence(logger, data):\r\n","\r\n","    spacings = [max([len(seq[i]) for seq in data.itervalues()]) for i in range(len(data[data.keys()[0]]))]\r\n","    for key, seq in data.iteritems():\r\n","        # logger.info(\"{} : \".format(key))\r\n","        to_print = \"\"\r\n","        for token, spacing in zip(seq, spacings):\r\n","            to_print += token + \" \" * (spacing - len(token) + 1)\r\n","        logger.info(to_print)\r\n","\r\n","\r\n","def get_logger(filename):\r\n","    logger = logging.getLogger('logger')\r\n","    logger.setLevel(logging.DEBUG)\r\n","    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\r\n","    handler = logging.FileHandler(filename)\r\n","    handler.setLevel(logging.DEBUG)\r\n","    handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\r\n","    logging.getLogger().addHandler(handler)\r\n","    return logger\r\n","\r\n","def batch_size_fn(new, count, sofar):\r\n","    global max_src_in_batch\r\n","    if count == 1:\r\n","        max_src_in_batch = 0\r\n","    max_src_in_batch = max(max_src_in_batch, len(new[1]) + 2)\r\n","    src_elements = count * max_src_in_batch\r\n","    return src_elements\r\n","\r\n","def report_func(epoch, batch, num_batches, start_time, report_stats,\r\n","                report_every, logger):\r\n","    if batch % report_every == -1 % report_every:\r\n","        report_stats.output(epoch, batch + 1, num_batches, start_time, logger)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdVXyamp9rcf"},"source":["#Models"]},{"cell_type":"code","metadata":{"id":"mCUgRkFJvBM2"},"source":["class Encoder(nn.Module):\r\n","    def __init__(self, config):\r\n","        super(Encoder, self).__init__()\r\n","        self.config = config\r\n","        self.rnn = nn.LSTM(input_size=config.encoder_input_size,\r\n","                           hidden_size=config.d_hidden,\r\n","                           num_layers=config.num_layers,\r\n","                           dropout=config.lstm_dropout,\r\n","                           bidirectional=config.brnn)\r\n","\r\n","    def forward(self, inputs, batch_size):\r\n","        memory_bank, encoder_final = self.rnn(inputs)\r\n","        return memory_bank, encoder_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkPWgHimvYWv"},"source":["class LstmPadding(object):\r\n","    def __init__(self, sent, sent_len, config):\r\n","        self.batch_size = len(sent_len)\r\n","        self.max_sent_len = max(sent_len)\r\n","        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\r\n","        self.idx_unsort = np.argsort(idx_sort)\r\n","        self.config = config\r\n","\r\n","        idx_sort = torch.from_numpy(idx_sort).type(LONG_TYPE)\r\n","        sent = sent.index_select(1, Variable(idx_sort))\r\n","\r\n","        self.sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len.copy())\r\n","\r\n","    def __call__(self, lstm_enc_func):\r\n","        idx_unsort = torch.from_numpy(self.idx_unsort). \\\r\n","            type(LONG_TYPE)\r\n","        memory_bank, enc_final = lstm_enc_func(self.sent_packed, self.batch_size)\r\n","\r\n","        enc_final = enc_final[0].index_select(1, Variable(idx_unsort)), \\\r\n","                    enc_final[1].index_select(1, Variable(idx_unsort))\r\n","\r\n","        memory_bank = nn.utils.rnn.pad_packed_sequence(memory_bank)[0]\r\n","        memory_bank = memory_bank.index_select(1, Variable(idx_unsort))\r\n","        memory_bank = memory_bank.transpose(0, 1).contiguous()\r\n","        return memory_bank, enc_final\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZrnV0VzgPxx"},"source":["class Progbar(object):\r\n","    def __init__(self, target, width=30, verbose=1):\r\n","        self.width = width\r\n","        self.target = target\r\n","        self.sum_values = {}\r\n","        self.unique_values = []\r\n","        self.start = time.time()\r\n","        self.total_width = 0\r\n","        self.seen_so_far = 0\r\n","        self.verbose = verbose\r\n","\r\n","    def update(self, current, values=[], exact=[], strict=[]):\r\n","\r\n","        for k, v in values:\r\n","            if k not in self.sum_values:\r\n","                self.sum_values[k] = [v * (current - self.seen_so_far), current - self.seen_so_far]\r\n","                self.unique_values.append(k)\r\n","            else:\r\n","                self.sum_values[k][0] += v * (current - self.seen_so_far)\r\n","                self.sum_values[k][1] += (current - self.seen_so_far)\r\n","        for k, v in exact:\r\n","            if k not in self.sum_values:\r\n","                self.unique_values.append(k)\r\n","            self.sum_values[k] = [v, 1]\r\n","\r\n","        for k, v in strict:\r\n","            if k not in self.sum_values:\r\n","                self.unique_values.append(k)\r\n","            self.sum_values[k] = v\r\n","\r\n","        self.seen_so_far = current\r\n","\r\n","        now = time.time()\r\n","        if self.verbose == 1:\r\n","            prev_total_width = self.total_width\r\n","            sys.stdout.write(\"\\b\" * prev_total_width)\r\n","            sys.stdout.write(\"\\r\")\r\n","\r\n","            numdigits = int(np.floor(np.log10(self.target))) + 1\r\n","            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\r\n","            bar = barstr % (current, self.target)\r\n","            prog = float(current)/self.target\r\n","            prog_width = int(self.width*prog)\r\n","            if prog_width > 0:\r\n","                bar += ('='*(prog_width-1))\r\n","                if current < self.target:\r\n","                    bar += '>'\r\n","                else:\r\n","                    bar += '='\r\n","            bar += ('.'*(self.width-prog_width))\r\n","            bar += ']'\r\n","            sys.stdout.write(bar)\r\n","            self.total_width = len(bar)\r\n","\r\n","            if current:\r\n","                time_per_unit = (now - self.start) / current\r\n","            else:\r\n","                time_per_unit = 0\r\n","            eta = time_per_unit*(self.target - current)\r\n","            info = ''\r\n","            if current < self.target:\r\n","                info += ' - ETA: %ds' % eta\r\n","            else:\r\n","                info += ' - %ds' % (now - self.start)\r\n","            for k in self.unique_values:\r\n","                if type(self.sum_values[k]) is list:\r\n","                    info += ' - %s: %.4f' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\r\n","                else:\r\n","                    info += ' - %s: %s' % (k, self.sum_values[k])\r\n","\r\n","            self.total_width += len(info)\r\n","            if prev_total_width > self.total_width:\r\n","                info += ((prev_total_width-self.total_width) * \" \")\r\n","\r\n","            sys.stdout.write(info)\r\n","            sys.stdout.flush()\r\n","\r\n","            if current >= self.target:\r\n","                sys.stdout.write(\"\\n\")\r\n","\r\n","        if self.verbose == 2:\r\n","            if current >= self.target:\r\n","                info = '%ds' % (now - self.start)\r\n","                for k in self.unique_values:\r\n","                    info += ' - %s: %.4f' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\r\n","                sys.stdout.write(info + \"\\n\")\r\n","\r\n","    def add(self, n, values=[]):\r\n","        self.update(self.seen_so_far+n, values)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyphhDK6fEna"},"source":["class RNNDecoderBase(nn.Module):\r\n","    def __init__(self, rnn_type, bidirectional_encoder, num_layers,\r\n","                 hidden_size, attn_type=\"general\", dropout=0.0,\r\n","                 embeddings=None):\r\n","        super(RNNDecoderBase, self).__init__()\r\n","\r\n","        self.decoder_type = 'rnn'\r\n","        self.bidirectional_encoder = bidirectional_encoder\r\n","        self.num_layers = num_layers\r\n","        self.hidden_size = hidden_size\r\n","        self.embeddings = embeddings\r\n","        self.dropout = nn.Dropout(dropout)\r\n","\r\n","        self.rnn = self._build_rnn(rnn_type,\r\n","                                   input_size=self._input_size,\r\n","                                   hidden_size=hidden_size,\r\n","                                   num_layers=num_layers,\r\n","                                   dropout=dropout)\r\n","        self.attn = GlobalAttention(hidden_size,\r\n","                                    attn_type=attn_type)\r\n","\r\n","    def forward(self, tgt, memory_bank, state, memory_lengths=None):\r\n","        assert isinstance(state, RNNDecoderState)\r\n","        tgt_len, tgt_batch = tgt.size()\r\n","        _, memory_batch, _ = memory_bank.size()\r\n","        aeq(tgt_batch, memory_batch)\r\n","  \r\n","        decoder_final, decoder_outputs, attns = self._run_forward_pass(\r\n","            tgt, memory_bank, state, memory_lengths=memory_lengths)\r\n","\r\n","        final_output = decoder_outputs[-1]\r\n","        coverage = None\r\n","        if \"coverage\" in attns:\r\n","            coverage = attns[\"coverage\"][-1].unsqueeze(0)\r\n","        state.update_state(decoder_final, final_output.unsqueeze(0), coverage)\r\n","\r\n","        return decoder_outputs, state, attns\r\n","\r\n","    def init_decoder_state(self, encoder_final):\r\n","        def _fix_enc_hidden(h):\r\n","            if self.bidirectional_encoder:\r\n","                h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\r\n","            return h\r\n","\r\n","        if isinstance(encoder_final, tuple):  # LSTM\r\n","            return RNNDecoderState(self.hidden_size,\r\n","                                   tuple([_fix_enc_hidden(enc_hid)\r\n","                                         for enc_hid in encoder_final]))\r\n","        else: \r\n","            return RNNDecoderState(self.hidden_size,\r\n","                                   _fix_enc_hidden(encoder_final))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHpyt6-O9rCY"},"source":["class ExponentialMovingAverage(object):\r\n","    def __init__(self, decay=0.999):\r\n","        self.decay = decay\r\n","        self.num_updates = 0\r\n","        self.shadow_variable_dict = {}\r\n","\r\n","    def register(self, var_list):\r\n","        for name, param in var_list.items():\r\n","            self.shadow_variable_dict[name] = param.clone()\r\n","\r\n","    def apply(self, var_list):\r\n","        self.num_updates += 1\r\n","        decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\r\n","        for name, param in var_list:\r\n","            if param.requires_grad:\r\n","                assert name in self.shadow_variable_dict\r\n","                data = self.shadow_variable_dict[name]\r\n","                data -= (1 - decay) * (data - param.data.clone())\r\n","\r\n","class SeqLabelReader(object):\r\n","    def __init__(self):\r\n","        pass\r\n","\r\n","    def build_vocab(self, files, **kwargs):\r\n","        pass\r\n","\r\n","    def load(self, filename, index, batchsz, **kwargs):\r\n","        pass\r\n","\r\n","\r\n","class TSVSeqLabelReader(SeqLabelReader):\r\n","    def __init__(self, mxlen=1000, mxfiltsz=0, vec_alloc=np.zeros):\r\n","        super(TSVSeqLabelReader, self).__init__()\r\n","\r\n","        self.vocab = None\r\n","        self.label2index = {}\r\n","        self.mxlen = mxlen\r\n","        self.mxfiltsz = mxfiltsz\r\n","        self.vec_alloc = vec_alloc\r\n","\r\n","    @staticmethod\r\n","    def splits(text):\r\n","        return text.lower().split()\r\n","\r\n","    @staticmethod\r\n","    def label_and_sentence(line):\r\n","        label_text = line.strip().lower().split('\\t')\r\n","        label = label_text[0]\r\n","        text = label_text[1:]\r\n","        text = ' '.join(text)\r\n","        return label, text\r\n","\r\n","    def build_vocab(self, files, **kwargs):\r\n","        label_idx = len(self.label2index)\r\n","        if type(files) == str:\r\n","            if os.path.isdir(files):\r\n","                base = files\r\n","                files = filter(os.path.isfile, [os.path.join(base, x) for x in os.listdir(base)])\r\n","            else:\r\n","                files = [files]\r\n","\r\n","        y = list()\r\n","        vocab = Counter()\r\n","        for file in files:\r\n","            if file is None:\r\n","                continue\r\n","            with io.open(file, encoding='utf-8', errors='ignore') as f:\r\n","                for line in tqdm(f):\r\n","                    label, text = TSVSeqLabelReader.label_and_sentence(line)\r\n","                    if label not in self.label2index:\r\n","                        self.label2index[label] = label_idx\r\n","                        label_idx += 1\r\n","                    for w in TSVSeqLabelReader.splits(text):\r\n","                        vocab[w] += 1\r\n","                    y.append(self.label2index[label])\r\n","\r\n","        if kwargs.get(\"class_weight\") == \"balanced\":\r\n","            class_weight = compute_class_weight(\"balanced\", list(self.label2index.values()), y)\r\n","        else:\r\n","            class_weight = None\r\n","\r\n","        return vocab, self.get_labels(), class_weight\r\n","\r\n","    def get_labels(self):\r\n","        labels = [''] * len(self.label2index)\r\n","        for label, index in self.label2index.items():\r\n","            labels[index] = label\r\n","        return labels\r\n","\r\n","    def load(self, filename, index, batchsz, **kwargs):\r\n","        PAD = index['<PAD>']\r\n","        shuffle = kwargs.get('shuffle', False)\r\n","        halffiltsz = self.mxfiltsz // 2\r\n","        nozplen = self.mxlen - 2 * halffiltsz\r\n","\r\n","        examples = []\r\n","        with io.open(filename, encoding='utf-8', errors='ignore') as f:\r\n","            for offset, line in enumerate(tqdm(f)):\r\n","                label, text = TSVSeqLabelReader.label_and_sentence(line)\r\n","                y = self.label2index[label]\r\n","                toks = TSVSeqLabelReader.splits(text)\r\n","                mx = min(len(toks), nozplen)\r\n","                toks = toks[:mx]\r\n","                x = self.vec_alloc(self.mxlen, dtype=int)\r\n","                for j in range(len(toks)):\r\n","                    w = toks[j]\r\n","                    key = index.get(w, PAD)\r\n","                    x[j + halffiltsz] = key\r\n","                examples.append((x, y))\r\n","\r\n","        return SeqLabelDataFeed(SeqLabelExamples(examples),batchsz=batchsz,\r\n","                                shuffle=shuffle,vec_alloc=self.vec_alloc,\r\n","                                src_vec_trans=None)\r\n","\r\n","\r\n","class SeqLabelExamples(object):\r\n","    SEQ = 0\r\n","    LABEL = 1\r\n","\r\n","    def __init__(self, example_list, do_shuffle=True):\r\n","        self.example_list = example_list\r\n","        if do_shuffle:\r\n","            random.shuffle(self.example_list)\r\n","\r\n","    def __getitem__(self, i):\r\n","        ex = self.example_list[i]\r\n","        return ex[SeqLabelExamples.SEQ], ex[SeqLabelExamples.LABEL]\r\n","\r\n","    def __len__(self):\r\n","        return len(self.example_list)\r\n","\r\n","    def width(self):\r\n","        x, y = self.example_list[0]\r\n","        return len(x)\r\n","\r\n","    def batch(self, start, batchsz, vec_alloc=np.empty):\r\n","        siglen = self.width()\r\n","        xb = vec_alloc((batchsz, siglen), dtype=np.int)\r\n","        yb = vec_alloc((batchsz), dtype=np.int)\r\n","        sz = len(self.example_list)\r\n","        idx = start * batchsz\r\n","        for i in range(batchsz):\r\n","            if idx >= sz:\r\n","                # idx = 0\r\n","                batchsz = i\r\n","                break\r\n","            x, y = self.example_list[idx]\r\n","            xb[i] = x\r\n","            yb[i] = y\r\n","            idx += 1\r\n","        return xb[: batchsz], yb[: batchsz]\r\n","\r\n","    @staticmethod\r\n","    def valid_split(data, splitfrac=0.15):\r\n","        numinst = len(data.examples)\r\n","        heldout = int(math.floor(numinst * (1 - splitfrac)))\r\n","        heldout_ex = data.example_list[1:heldout]\r\n","        rest_ex = data.example_list[heldout:]\r\n","        return SeqLabelExamples(heldout_ex), SeqLabelExamples(rest_ex)\r\n","\r\n","\r\n","class DataFeed(object):\r\n","    def __init__(self):\r\n","        self.steps = 0\r\n","        self.shuffle = False\r\n","\r\n","    def _batch(self, i):\r\n","        pass\r\n","\r\n","    def __getitem__(self, i):\r\n","        return self._batch(i)\r\n","\r\n","    def __iter__(self):\r\n","        shuffle = np.random.permutation(np.arange(self.steps)) if self.shuffle else np.arange(self.steps)\r\n","\r\n","        for i in range(self.steps):\r\n","            si = shuffle[i]\r\n","            yield self._batch(si)\r\n","\r\n","    def __len__(self):\r\n","        return self.steps\r\n","\r\n","\r\n","class ExampleDataFeed(DataFeed):\r\n","\r\n","    def __init__(self, examples, batchsz, **kwargs):\r\n","        super(ExampleDataFeed, self).__init__()\r\n","\r\n","        self.examples = examples\r\n","        self.batchsz = batchsz\r\n","        self.shuffle = bool(kwargs.get('shuffle', False))\r\n","        self.vec_alloc = kwargs.get('vec_alloc', np.zeros)\r\n","        self.vec_shape = kwargs.get('vec_shape', np.shape)\r\n","        self.src_vec_trans = kwargs.get('src_vec_trans', None)\r\n","        # self.steps = int(math.floor(len(self.examples) / float(batchsz)))\r\n","        self.steps = int(math.ceil(len(self.examples) / float(batchsz)))\r\n","        self.trim = bool(kwargs.get('trim', False))\r\n","\r\n","\r\n","class SeqLabelDataFeed(ExampleDataFeed):\r\n","    def __init__(self, examples, batchsz, **kwargs):\r\n","        super(SeqLabelDataFeed, self).__init__(examples, batchsz, **kwargs)\r\n","\r\n","    def _batch(self, i):\r\n","        x, y = self.examples.batch(i, self.batchsz, self.vec_alloc)\r\n","        if self.src_vec_trans is not None:\r\n","            x = self.src_vec_trans(x)\r\n","        return x, y\r\n","\r\n","class WeightDrop(torch.nn.Module):\r\n","    def __init__(self, module, weights, dropout=0, variational=False):\r\n","        super(WeightDrop, self).__init__()\r\n","        self.module = module\r\n","        self.weights = weights\r\n","        self.dropout = dropout\r\n","        self.variational = variational\r\n","        self._setup()\r\n","\r\n","    def widget_demagnetizer_y2k_edition(*args, **kwargs):\r\n","        return\r\n","\r\n","    def _setup(self)\r\n","        if issubclass(type(self.module), torch.nn.RNNBase):\r\n","            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\r\n","\r\n","        for name_w in self.weights:\r\n","            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\r\n","            w = getattr(self.module, name_w)\r\n","            del self.module._parameters[name_w]\r\n","            self.module.register_parameter(name_w + '_raw',\r\n","                                           torch.nn.Parameter(w.data))\r\n","\r\n","    def _setweights(self):\r\n","        for name_w in self.weights:\r\n","            raw_w = getattr(self.module, name_w + '_raw')\r\n","            w = None\r\n","            if self.variational:\r\n","                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\r\n","                if raw_w.is_cuda: mask = mask.cuda()\r\n","                mask = torch.nn.functional.dropout(mask, p=self.dropout,\r\n","                                                   training=True)\r\n","                w = mask.expand_as(raw_w) * raw_w\r\n","            else:\r\n","                w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\r\n","            setattr(self.module, name_w, w)\r\n","\r\n","    def forward(self, *args):\r\n","        self._setweights()\r\n","        return self.module.forward(*args)\r\n","\r\n","\r\n","\r\n","class AdaptiveDropout(torch.nn.Module):\r\n","    def __init__(self):\r\n","        super(AdaptiveDropout, self).__init__()\r\n","\r\n","    def forward(self, input, p):\r\n","        if self.training:\r\n","            p = 1. - p.data\r\n","            temp = torch.rand(input.size()).cuda() < p\r\n","            temp = torch.autograd.Variable(temp.type_as(p) / p)\r\n","            input = torch.mul(input, temp)\r\n","            return input\r\n","        else:\r\n","            return input\r\n","\r\n","\r\n","class LockedDropout(torch.nn.Module):\r\n","    def __init__(self, dropout=None):\r\n","        super(LockedDropout, self).__init__()\r\n","        self.dropout = dropout\r\n","\r\n","    def forward(self, x):\r\n","        if not self.training or not self.dropout:\r\n","            return x\r\n","        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout)\r\n","        mask = torch.autograd.Variable(m, requires_grad=False) / (\r\n","                    1 - self.dropout)\r\n","        mask = mask.expand_as(x)\r\n","        return mask * x\r\n","\r\n","class SequenceCriteria(nn.Module):\r\n","    def __init__(self, class_weight):\r\n","        super(SequenceCriteria, self).__init__()\r\n","        self.criteria = nn.CrossEntropyLoss(weight=class_weight)\r\n","\r\n","    def forward(self, inputs, targets):\r\n","        # This is BxT, which is what we want!\r\n","        loss = self.criteria(inputs, targets)\r\n","        return loss\r\n","\r\n","\r\n","class Discriminator(nn.Module):\r\n","    def __init__(self, config):\r\n","        super(Discriminator, self).__init__()\r\n","        self.config = config\r\n","        seq_in_size = config.d_hidden\r\n","        if config.brnn:\r\n","            seq_in_size *= 2\r\n","      \r\n","        layers = [nn.Dropout(0.3),\r\n","                  nn.Linear(seq_in_size, 1024),\r\n","                  nn.LeakyReLU()]\r\n","        for _ in range(config.num_discriminator_layers - 1):\r\n","            layers.append(nn.Dropout(0.3))\r\n","            layers.append(nn.Linear(1024, 1024))\r\n","            layers.append(nn.LeakyReLU())\r\n","        layers.append(nn.Dropout(0.3))\r\n","        layers.append(nn.Linear(1024, 1))\r\n","        self.model = nn.Sequential(*layers)\r\n","\r\n","    def forward(self, x, sequence_lengths, ids_, smoothing=0.0):\r\n","        B, T, D = x.shape\r\n","        y = self.model(torch.max(x.transpose(1, 2).contiguous(), 2)[0])\r\n","        y = F.sigmoid(y)\r\n","        loss = binary_cross_entropy(y, ids_, smoothing=smoothing)\r\n","        return loss\r\n","\r\n","class Classifier(nn.Module):\r\n","    def __init__(self, config):\r\n","        super(Classifier, self).__init__()\r\n","        self.config = config\r\n","        seq_in_size = config.d_hidden\r\n","        if config.brnn:\r\n","            seq_in_size *= 2\r\n","        if config.down_projection:\r\n","            self.down_projection = _linear(seq_in_size,\r\n","                                           config.d_down_proj,\r\n","                                           config.init_scalar)\r\n","            self.act = nn.ReLU()\r\n","            seq_in_size = config.d_down_proj\r\n","        self.clf = _linear(seq_in_size, config.num_classes, config.init_scalar)\r\n","\r\n","    def forward(self, x):\r\n","        x = x.transpose(1, 2).contiguous()\r\n","        if self.config.pool_type == \"max_pool\":\r\n","            sent_output = torch.max(x, 2)[0]\r\n","        elif self.config.pool_type == \"avg_pool\":\r\n","            normalize = 1. / np.sqrt(self.max_sent_len)\r\n","            sent_output = torch.sum(x, 2).mul_(normalize)\r\n","        if self.config.down_projection:\r\n","            sent_output = self.act(self.down_projection(sent_output))\r\n","        logits = self.clf(sent_output)\r\n","        return logits\r\n","\r\n","\r\n","class Embedder(nn.Module):\r\n","    def __init__(self, config):\r\n","        super(Embedder, self).__init__()\r\n","        self.config = config\r\n","        self.embedder = nn.Embedding(config.n_vocab,\r\n","                                     config.d_units,\r\n","                                     padding_idx=Vocab_Pad.PAD)\r\n","        \r\n","        if config.use_pretrained_embeddings:\r\n","            print(\"Loading pre-trained word vectors\")\r\n","            embeddings = wordvector\r\n","            self.embedder.weight = torch.nn.Parameter(torch.from_numpy(embeddings),\r\n","                                                      requires_grad=config.train_embeddings)\r\n","        if config.adaptive_dropout:\r\n","            self.word_dropout = LockedDropout(dropout=config.locked_dropout)\r\n","        else:\r\n","            self.word_dropout = nn.Dropout(p=config.word_dropout)\r\n","\r\n","    def _normalize(self, emb):\r\n","        weights = self.vocab_freqs / torch.sum(self.vocab_freqs)\r\n","        weights = weights.unsqueeze(-1)\r\n","        mean = torch.sum(weights * emb, 0, keepdim=True)\r\n","        var = torch.sum(weights * torch.pow(emb - mean, 2.), 0, keepdim=True)\r\n","        stddev = torch.sqrt(1e-6 + var)\r\n","        return (emb - mean) / stddev\r\n","\r\n","    def forward(self, batch):\r\n","        if self.config.normalize_embedding:\r\n","            self.embedder.weight.data = self._normalize(self.embedder.weight.data)\r\n","\r\n","        if self.config.adaptive_dropout:\r\n","            word_embedding = embedded_dropout(self.embedder,\r\n","                                          batch.word_ids,\r\n","                                          dropout=self.config.word_dropout\r\n","                                          if self.training else 0)\r\n","            dropped = self.word_dropout(word_embedding.transpose(0, 1).contiguous()).transpose(0, 1).contiguous()\r\n","        else:\r\n","            word_embedding = self.embedder(batch.word_ids)\r\n","            dropped = self.word_dropout(word_embedding)\r\n","        return dropped\r\n","\r\n","class LSTMEncoder(nn.Module):\r\n","    def __init__(self, config):\r\n","        super(LSTMEncoder, self).__init__()\r\n","        self.dropout = nn.Dropout(p=config.dropout)\r\n","        if config.projection:\r\n","            self.projection = nn.Linear(config.d_units, config.d_proj)\r\n","            self.act1 = nn.ReLU()\r\n","        config.encoder_input_size = config.d_proj \\\r\n","            if config.projection else config.d_units\r\n","\r\n","        self.lstm_encoder = Encoder(config)\r\n","        seq_in_size = config.d_hidden\r\n","        if config.brnn:\r\n","            seq_in_size *= 2\r\n","        self.lstm_dropout = nn.Dropout(config.lstm_dropout)\r\n","        self.config = config\r\n","\r\n","    def encode_sent(self, embedded, sent_len):\r\n","        if self.config.projection:\r\n","            embedded = self.act1(self.projection(embedded))\r\n","        memory_bank, encoder_final = LstmPadding(embedded,\r\n","                                                 sent_len,\r\n","                                                 self.config)(self.lstm_encoder)\r\n","        return memory_bank, encoder_final\r\n","\r\n","    def forward(self, embedded, batch, *args, **kwargs):\r\n","        memory_bank, encoder_final = self.encode_sent(embedded,\r\n","                                                      batch.sent_len)\r\n","        memory_bank = self.lstm_dropout(memory_bank)\r\n","        return memory_bank, encoder_final\r\n","\r\n","\r\n","class DecoderState(object):\r\n","    def detach(self):\r\n","        for h in self._all:\r\n","            if h is not None:\r\n","                h.detach_()\r\n","\r\n","    def beam_update(self, idx, positions, beam_size):\r\n","        for e in self._all:\r\n","            sizes = e.size()\r\n","            br = sizes[1]\r\n","            if len(sizes) == 3:\r\n","                sent_states = e.view(sizes[0], beam_size, br // beam_size,\r\n","                                     sizes[2])[:, :, idx]\r\n","            else:\r\n","                sent_states = e.view(sizes[0], beam_size,\r\n","                                     br // beam_size,\r\n","                                     sizes[2],\r\n","                                     sizes[3])[:, :, idx]\r\n","\r\n","            sent_states.data.copy_(\r\n","                sent_states.data.index_select(1, positions))\r\n","\r\n","\r\n","class RNNDecoderState(DecoderState):\r\n","    def __init__(self, hidden_size, rnnstate):\r\n","        if not isinstance(rnnstate, tuple):\r\n","            self.hidden = (rnnstate,)\r\n","        else:\r\n","            self.hidden = rnnstate\r\n","        self.coverage = None\r\n","\r\n","        # Init the input feed.\r\n","        batch_size = self.hidden[0].size(1)\r\n","        h_size = (batch_size, hidden_size)\r\n","        self.input_feed = Variable(self.hidden[0].data.new(*h_size).zero_(),\r\n","                                   requires_grad=False).unsqueeze(0)\r\n","\r\n","    @property\r\n","    def _all(self):\r\n","        return self.hidden + (self.input_feed,)\r\n","\r\n","    def update_state(self, rnnstate, input_feed, coverage):\r\n","        if not isinstance(rnnstate, tuple):\r\n","            self.hidden = (rnnstate,)\r\n","        else:\r\n","            self.hidden = rnnstate\r\n","        self.input_feed = input_feed\r\n","        self.coverage = coverage\r\n","\r\n","    def repeat_beam_size_times(self, beam_size):\r\n","        vars = [Variable(e.data.repeat(1, beam_size, 1), volatile=True)\r\n","                for e in self._all]\r\n","        self.hidden = tuple(vars[:-1])\r\n","        self.input_feed = vars[-1]\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","class Statistics(object):\r\n","    def __init__(self):\r\n","        self.clf_loss = 0\r\n","        self.ae_loss = 0.\r\n","        self.at_loss = 0.\r\n","        self.vat_loss = 0.\r\n","        self.entropy_loss = 0.\r\n","        self.n_words = 0\r\n","        self.n_correct = 0\r\n","        self.n_sent = 0\r\n","        self.grad_norm = 0\r\n","        self.start_time = time.time()\r\n","\r\n","    def accuracy(self):\r\n","        return 100 * (self.n_correct / self.n_sent)\r\n","\r\n","    def elapsed_time(self):\r\n","        return time.time() - self.start_time\r\n","\r\n","    def output(self, epoch, batch, n_batches, start, logger):\r\n","        t = self.elapsed_time()\r\n","        logger.info((\"Epoch %2d, %5d/%5d; \"\r\n","                     \"acc: %6.2f; \"\r\n","                     \"clf_loss: %1.4f; \"\r\n","                     \"at_loss: %1.4f; \"\r\n","                     \"vat_loss: %1.4f; \"\r\n","                     \"entropy_loss: %1.4f; \"\r\n","                     \"ae_loss: %1.4f; \"\r\n","                     \"norm: %2.4f; \"\r\n","                     \"%3.0f tok/s; \"\r\n","                     \"%6.0f s elapsed\") %\r\n","                    (epoch,\r\n","                     batch,\r\n","                     n_batches,\r\n","                     self.accuracy(),\r\n","                     self.clf_loss / (batch + 1),\r\n","                     self.at_loss / (batch + 1),\r\n","                     self.vat_loss / (batch + 1),\r\n","                     self.entropy_loss / (batch + 1),\r\n","                     self.ae_loss / (batch + 1),\r\n","                     self.grad_norm / (batch + 1),\r\n","                     self.n_words / (t + 1e-5),\r\n","                     time.time() - start))\r\n","        sys.stdout.flush()\r\n","\r\n","    def log(self, prefix, experiment, lr):\r\n","        t = self.elapsed_time()\r\n","        experiment.add_scalar_value(prefix + \"_accuracy\", self.accuracy())\r\n","        experiment.add_scalar_value(prefix + \"_tgtper\", self.n_words / t)\r\n","        experiment.add_scalar_value(prefix + \"_lr\", lr)\r\n","\r\n","\r\n","\r\n","class PrettyMetrics(float):\r\n","    def __repr__(self):\r\n","        return \"%0.2f\" % (self)\r\n","\r\n","\r\n","class ConfusionMatrix(object):\r\n","    def __init__(self, labels):\r\n","        if type(labels) is dict:\r\n","            self.labels = []\r\n","            for i in range(len(labels)):\r\n","                self.labels.append(labels[i])\r\n","        else:\r\n","            self.labels = labels\r\n","        nc = len(self.labels)\r\n","        self._cm = np.zeros((nc, nc), dtype=np.int)\r\n","\r\n","    def add(self, truth, guess):\r\n","        self._cm[truth, guess] += 1\r\n","\r\n","    def __str__(self):\r\n","        values = []\r\n","        width = max(8, max(len(x) for x in self.labels) + 1)\r\n","        for i, label in enumerate([''] + self.labels):\r\n","            values += [\"{:>{width}}\".format(label, width=width + 1)]\r\n","        values += ['\\n']\r\n","        for i, label in enumerate(self.labels):\r\n","            values += [\"{:>{width}}\".format(label, width=width + 1)]\r\n","            for j in range(len(self.labels)):\r\n","                values += [\"{:{width}d}\".format(self._cm[i, j], width=width + 1)]\r\n","            values += ['\\n']\r\n","        values += ['\\n']\r\n","        return ''.join(values)\r\n","\r\n","    def reset(self):\r\n","        self._cm *= 0\r\n","\r\n","    def get_correct(self):\r\n","        return self._cm.diagonal().sum()\r\n","\r\n","    def get_total(self):\r\n","        return self._cm.sum()\r\n","\r\n","    def get_acc(self):\r\n","        return float(self.get_correct()) / self.get_total()\r\n","\r\n","    def get_recall(self):\r\n","        total = np.sum(self._cm, axis=1) + 0.0000001\r\n","        return np.diag(self._cm) / total\r\n","\r\n","    def get_precision(self):\r\n","        total = np.sum(self._cm, axis=0) + 0.0000001\r\n","        return np.diag(self._cm) / total\r\n","\r\n","    def get_mean_precision(self):\r\n","        return np.mean(self.get_precision())\r\n","\r\n","    def get_mean_recall(self):\r\n","        return np.mean(self.get_recall())\r\n","\r\n","    def get_macro_f(self, beta=1):\r\n","        p = self.get_mean_precision()\r\n","        r = self.get_mean_recall()\r\n","        if beta < 0:\r\n","            raise Exception('Beta must be greater than 0')\r\n","        return (beta * beta + 1) * p * r / (beta * beta * p + r)\r\n","\r\n","    def get_f(self, beta=1):\r\n","        p = self.get_precision()[1]\r\n","        r = self.get_recall()[1]\r\n","        if beta < 0:\r\n","            raise Exception('Beta must be greater than 0')\r\n","        return (beta * beta + 1) * p * r / (beta * beta * p + r)\r\n","\r\n","    def get_all_metrics(self):\r\n","        metrics = {}\r\n","        metrics['acc'] = PrettyMetrics(self.get_acc() * 100)\r\n","        metrics['correct'] = PrettyMetrics(self.get_correct())\r\n","        metrics['total'] = PrettyMetrics(self.get_total())\r\n","        if len(self.labels) == 2:\r\n","            metrics['precision'] = PrettyMetrics(self.get_precision()[1] * 100)\r\n","            metrics['recall'] = PrettyMetrics(self.get_recall()[1] * 100)\r\n","            metrics['f1'] = PrettyMetrics(self.get_f(1) * 100)\r\n","        else:\r\n","            metrics['mean_precision'] = PrettyMetrics(self.get_mean_precision() * 100)\r\n","            metrics['mean_recall'] = PrettyMetrics(self.get_mean_recall() * 100)\r\n","            metrics['macro_f1'] = PrettyMetrics(self.get_macro_f(1) * 100)\r\n","        return metrics\r\n","\r\n","    def add_batch(self, truth, guess):\r\n","        for truth_i, guess_i in zip(truth, guess):\r\n","            self.add(truth_i, guess_i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4IMCBVBAiv1"},"source":["class AEModel(nn.Module):\r\n","\r\n","    def __init__(self, config):\r\n","        super(AEModel, self).__init__()\r\n","        self.config = config\r\n","        self.embed = nn.Embedding(config.n_vocab,\r\n","                                  config.d_units,\r\n","                                  padding_idx=Vocab_Pad.PAD)\r\n","        self.decoder = StdRNNDecoder(rnn_type='LSTM',\r\n","                                     bidirectional_encoder=True,\r\n","                                     num_layers=1,\r\n","                                     hidden_size=config.hidden_size,\r\n","                                     dropout=0.2,\r\n","                                     embeddings=self.embed,\r\n","                                     attn_type=\"general\")\r\n","        self.affine = nn.Linear(config.hidden_size,\r\n","                                config.n_vocab,\r\n","                                bias=True)\r\n","        weight = torch.ones(config.n_vocab)\r\n","        weight[Vocab_Pad.PAD] = 0\r\n","        self.criterion = nn.NLLLoss(weight,\r\n","                                    size_average=False)\r\n","\r\n","    def output_and_loss(self, h_block, t_block):\r\n","        batch, length, units = h_block.shape\r\n","        logits_flat = seq_func(self.affine,\r\n","                               h_block,\r\n","                               reconstruct_shape=False)\r\n","        log_probs_flat = F.log_softmax(logits_flat,\r\n","                                       dim=-1)\r\n","        rebatch, _ = logits_flat.shape\r\n","        concat_t_block = t_block.view(rebatch)\r\n","        weights = (concat_t_block >= 1).float()\r\n","\r\n","        loss = self.criterion(log_probs_flat,\r\n","                              concat_t_block)\r\n","        loss = loss.sum() / (weights.sum() + 1e-13)\r\n","        return loss\r\n","\r\n","    def forward(self, memory_bank, enc_final, lengths, ly_batch_raw,\r\n","                dec_state=None):\r\n","\r\n","        tgt_in_block, tgt_out_block = seq2seq_pad_concat(\r\n","            ly_batch_raw, -1)\r\n","        tgt_in_block = tgt_in_block.transpose(0, 1).contiguous()\r\n","\r\n","        memory_bank = memory_bank.transpose(0, 1).contiguous()\r\n","        lengths = torch.from_numpy(lengths).type(LONG_TYPE)\r\n","        enc_state = self.decoder.init_decoder_state(enc_final)\r\n","        decoder_outputs, dec_state, attns = self.decoder(tgt_in_block,\r\n","                                                         memory_bank,\r\n","                                                         enc_state if dec_state is None\r\n","                                                         else dec_state,\r\n","                                                         memory_lengths=lengths)\r\n","        decoder_outputs = decoder_outputs.transpose(0, 1).contiguous()\r\n","        loss = self.output_and_loss(decoder_outputs, tgt_out_block)\r\n","        return loss\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43H4cHQaX7xW"},"source":["class StdRNNDecoder(RNNDecoderBase):\r\n","    def _run_forward_pass(self, tgt, memory_bank, state, memory_lengths=None):\r\n","        attns = {}\r\n","        emb = self.embeddings(tgt)\r\n","\r\n","        if isinstance(self.rnn, nn.GRU):\r\n","            rnn_output, decoder_final = self.rnn(emb, state.hidden[0])\r\n","        else:\r\n","            rnn_output, decoder_final = self.rnn(emb, state.hidden)\r\n","\r\n","        tgt_len, tgt_batch = tgt.size()\r\n","        output_len, output_batch, _ = rnn_output.size()\r\n","        aeq(tgt_len, output_len)\r\n","        aeq(tgt_batch, output_batch)\r\n","\r\n","        decoder_outputs, p_attn = self.attn(\r\n","            rnn_output.transpose(0, 1).contiguous(),\r\n","            memory_bank.transpose(0, 1),\r\n","            memory_lengths=memory_lengths\r\n","        )\r\n","        attns[\"std\"] = p_attn\r\n","\r\n","        decoder_outputs = self.dropout(decoder_outputs)\r\n","        return decoder_final, decoder_outputs, attns\r\n","\r\n","    def _build_rnn(self, rnn_type, **kwargs):\r\n","        rnn, _ = rnn_factory(rnn_type, **kwargs)\r\n","        return rnn\r\n","\r\n","    @property\r\n","    def _input_size(self):\r\n","        return self.embeddings.embedding_dim\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxVfwwqI6XYd"},"source":["class Training(object):\r\n","    def __init__(self, config, logger=None):\r\n","        if logger is None:\r\n","            logger = logging.getLogger('logger')\r\n","            logger.setLevel(logging.DEBUG)\r\n","            logging.basicConfig(format='%(message)s', level=logging.DEBUG)\r\n","\r\n","        self.logger = logger\r\n","        self.config = config\r\n","        self.classes = list(config.id2label.keys())\r\n","        self.num_classes = config.num_classes\r\n","\r\n","        self.embedder = Embedder(self.config).to(device)\r\n","        self.encoder = LSTMEncoder(self.config).to(device)\r\n","        self.clf = Classifier(self.config).to(device)\r\n","        self.clf_loss = SequenceCriteria(class_weight=None).to(device)\r\n","        if self.config.lambda_ae > 0: self.ae = AEModel(self.config).to(device)\r\n","\r\n","        self.writer = SummaryWriter(log_dir=\"TFBoardSummary\")\r\n","        self.global_steps = 0\r\n","        \r\n","        self.enc_clf_opt = Adam(self._get_trainabe_modules(),lr=self.config.lr,\r\n","                                betas=(config.beta1,config.beta2),weight_decay=config.weight_decay,\r\n","                                eps=config.eps)\r\n","\r\n","        if config.scheduler == \"ReduceLROnPlateau\":\r\n","            self.scheduler = lr_scheduler.ReduceLROnPlateau(self.enc_clf_opt,mode='max',\r\n","                                                            factor=config.lr_decay,patience=config.patience,\r\n","                                                            verbose=True)\r\n","        elif config.scheduler == \"ExponentialLR\":\r\n","            self.scheduler = lr_scheduler.ExponentialLR(self.enc_clf_opt,gamma=config.gamma)\r\n","\r\n","        self._init_or_load_model()\r\n","        if config.multi_gpu:\r\n","            self.embedder.cuda()\r\n","            self.encoder.cuda()\r\n","            self.clf.cuda()\r\n","            self.clf_loss.cuda()\r\n","            if self.config.lambda_ae > 0: self.ae.cuda()\r\n","\r\n","        self.ema_embedder = ExponentialMovingAverage(decay=0.999)\r\n","        self.ema_embedder.register(self.embedder.state_dict())\r\n","        self.ema_encoder = ExponentialMovingAverage(decay=0.999)\r\n","        self.ema_encoder.register(self.encoder.state_dict())\r\n","        self.ema_clf = ExponentialMovingAverage(decay=0.999)\r\n","        self.ema_clf.register(self.clf.state_dict())\r\n","\r\n","        self.time_s = time.time()\r\n","\r\n","    def _get_trainabe_modules(self):\r\n","        param_list = list(self.embedder.parameters()) + \\\r\n","                     list(self.encoder.parameters()) + \\\r\n","                     list(self.clf.parameters())\r\n","        if self.config.lambda_ae > 0:\r\n","            param_list += list(self.ae.parameters())\r\n","        return param_list\r\n","\r\n","    def _get_l2_norm_loss(self):\r\n","        total_norm = 0.\r\n","        for p in self._get_trainabe_modules():\r\n","            param_norm = p.data.norm(p=2)\r\n","            total_norm += param_norm  \r\n","        return total_norm  \r\n","\r\n","    def _init_or_load_model(self):\r\n","        ensure_directory(self.config.output_path)\r\n","        self.epoch = 0\r\n","        self.best_accuracy = -np.inf\r\n","\r\n","    def _compute_vocab_freq(self, train_, dev_):\r\n","        counter = collections.Counter()\r\n","        for _, ids_ in train_:\r\n","            counter.update(ids_)\r\n","        for _, ids_ in dev_:\r\n","            counter.update(ids_)\r\n","        word_freq = np.zeros(self.config.n_vocab)\r\n","        for index_, freq_ in counter.items():\r\n","            word_freq[index_] = freq_\r\n","        return torch.from_numpy(word_freq).type(FLOAT_TYPE)\r\n","\r\n","    def _save_model(self):\r\n","        state = {'epoch': self.epoch,\r\n","                 'state_dict_encoder': self.ema_encoder.shadow_variable_dict,\r\n","                 'state_dict_embedder': self.ema_embedder.shadow_variable_dict,\r\n","                 'state_dict_clf': self.ema_clf.shadow_variable_dict,\r\n","                 'best_accuracy': self.best_accuracy}\r\n","        torch.save(state, os.path.join(self.config.output_path,\r\n","                                       self.config.model_file))\r\n","\r\n","    def _load_model(self):\r\n","        checkpoint_path = os.path.join(self.config.output_path,\r\n","                                       self.config.model_file)\r\n","        if self.config.load_checkpoint and os.path.isfile(checkpoint_path):\r\n","            dict_ = torch.load(checkpoint_path)\r\n","            self.epoch = dict_['epoch']\r\n","            self.best_accuracy = dict_['best_accuracy']\r\n","            self.embedder.load_state_dict(dict_['state_dict_embedder'])\r\n","            self.encoder.load_state_dict(dict_['state_dict_encoder'])\r\n","            self.clf.load_state_dict(dict_['state_dict_clf'])\r\n","            self.logger.info(\r\n","                \"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_path,\r\n","                                                              self.epoch))\r\n","            return True\r\n","\r\n","    def __call__(self, train, dev, test, unlabel):\r\n","        if self.config.normalize_embedding:\r\n","            self.embedder.vocab_freqs = self._compute_vocab_freq(train, dev)\r\n","            print(\"Embeddings will be normalized during training\")\r\n","        self.logger.info('Start training')\r\n","        self._train(train, dev, unlabel)\r\n","        self._evaluate(test)\r\n","\r\n","    def _create_iter(self, data_, wbatchsize, random_shuffler=data.iterator.RandomShuffler()):\r\n","        iter_ = data.iterator.pool(data_,\r\n","                                   wbatchsize,\r\n","                                   key=lambda x: len(x[1]),\r\n","                                   batch_size_fn=batch_size_fn,\r\n","                                   random_shuffler=random_shuffler\r\n","                                   )\r\n","        return iter_\r\n","\r\n","    def _run_epoch(self, train_data, dev_data, unlabel_data):\r\n","        report_stats = Statistics()\r\n","        cm = ConfusionMatrix(self.classes)\r\n","        _, seq_data = list(zip(*train_data))\r\n","        total_seq_words = len(list(itertools.chain.from_iterable(seq_data)))\r\n","        iter_per_epoch = (1.5 * total_seq_words) // self.config.wbatchsize\r\n","\r\n","        self.encoder.train()\r\n","        self.clf.train()\r\n","        self.embedder.train()\r\n","        train_iter = self._create_iter(train_data, self.config.wbatchsize)\r\n","        unlabel_iter = self._create_iter(unlabel_data,\r\n","                                         self.config.wbatchsize_unlabel)\r\n","        for batch_index, train_batch_raw in enumerate(train_iter):\r\n","            seq_iter = list(zip(*train_batch_raw))[1]\r\n","            seq_words = len(list(itertools.chain.from_iterable(seq_iter)))\r\n","            report_stats.n_words += seq_words\r\n","            self.global_steps += 1\r\n","\r\n","            if self.config.add_noise:\r\n","                train_batch_raw = add_noise(train_batch_raw,\r\n","                                            self.config.noise_dropout,\r\n","                                            self.config.random_permutation)\r\n","            train_batch = seq_pad_concat(train_batch_raw, -1)\r\n","\r\n","            train_embedded = self.embedder(train_batch)\r\n","            memory_bank_train, enc_final_train = self.encoder(train_embedded, train_batch)\r\n","\r\n","            if self.config.lambda_vat > 0 or self.config.lambda_ae > 0 or self.config.lambda_entropy:\r\n","                try:\r\n","                    unlabel_batch_raw = next(unlabel_iter)\r\n","                except StopIteration:\r\n","                    unlabel_iter = self._create_iter(unlabel_data,\r\n","                                                     self.config.wbatchsize_unlabel)\r\n","                    unlabel_batch_raw = next(unlabel_iter)\r\n","\r\n","                if self.config.add_noise:\r\n","                    unlabel_batch_raw = add_noise(unlabel_batch_raw,\r\n","                                                  self.config.noise_dropout,\r\n","                                                  self.config.random_permutation)\r\n","                unlabel_batch = seq_pad_concat(unlabel_batch_raw,-1)\r\n","                unlabel_embedded = self.embedder(unlabel_batch)\r\n","                memory_bank_unlabel, enc_final_unlabel = self.encoder(\r\n","                    unlabel_embedded,\r\n","                    unlabel_batch)\r\n","\r\n","            pred = self.clf(memory_bank_train)\r\n","            accuracy = self.get_accuracy(cm, pred.data, train_batch.labels.data)\r\n","            lclf = self.clf_loss(pred, train_batch.labels)\r\n","\r\n","            lat = Variable(torch.FloatTensor([-1.]).type(FLOAT_TYPE))\r\n","            lvat = Variable(torch.FloatTensor([-1.]).type(FLOAT_TYPE))\r\n","            if self.config.lambda_at > 0:\r\n","                lat = at_loss(self.embedder,\r\n","                              self.encoder,\r\n","                              self.clf,\r\n","                              train_batch,\r\n","                              perturb_norm_length=self.config.perturb_norm_length)\r\n","\r\n","            if self.config.lambda_vat > 0:\r\n","                lvat_train = vat_loss(self.embedder,\r\n","                                      self.encoder,\r\n","                                      self.clf,\r\n","                                      train_batch,\r\n","                                      p_logit=pred,\r\n","                                      perturb_norm_length=self.config.perturb_norm_length)\r\n","                if self.config.inc_unlabeled_loss:\r\n","                    lvat_unlabel = vat_loss(self.embedder,\r\n","                                            self.encoder,\r\n","                                            self.clf,\r\n","                                            unlabel_batch,\r\n","                                            p_logit=self.clf(memory_bank_unlabel),\r\n","                                            perturb_norm_length=self.config.perturb_norm_length)\r\n","                    if self.config.unlabeled_loss_type == \"AvgTrainUnlabel\":\r\n","                        lvat = 0.5 * (lvat_train + lvat_unlabel)\r\n","                    elif self.config.unlabeled_loss_type == \"Unlabel\":\r\n","                        lvat = lvat_unlabel\r\n","                else:\r\n","                    lvat = lvat_train\r\n","\r\n","            lentropy = Variable(torch.FloatTensor([-1.]).type(FLOAT_TYPE))\r\n","            if self.config.lambda_entropy > 0:\r\n","                lentropy_train = entropy_loss(pred)\r\n","                if self.config.inc_unlabeled_loss:\r\n","                    lentropy_unlabel = entropy_loss(self.clf(memory_bank_unlabel))\r\n","                    if self.config.unlabeled_loss_type == \"AvgTrainUnlabel\":\r\n","                        lentropy = 0.5 * (lentropy_train + lentropy_unlabel)\r\n","                    elif self.config.unlabeled_loss_type == \"Unlabel\":\r\n","                        lentropy = lentropy_unlabel\r\n","                else:\r\n","                    lentropy = lentropy_train\r\n","\r\n","            lae = Variable(torch.FloatTensor([-1.]).type(FLOAT_TYPE))\r\n","            if self.config.lambda_ae > 0:\r\n","                lae = self.ae(memory_bank_unlabel,\r\n","                              enc_final_unlabel,\r\n","                              unlabel_batch.sent_len,\r\n","                              unlabel_batch_raw)\r\n","\r\n","            ltotal = (self.config.lambda_clf * lclf) + \\\r\n","                     (self.config.lambda_ae * lae) + \\\r\n","                     (self.config.lambda_at * lat) + \\\r\n","                     (self.config.lambda_vat * lvat) + \\\r\n","                     (self.config.lambda_entropy * lentropy)\r\n","\r\n","            report_stats.clf_loss += lclf.data.cpu().numpy()\r\n","            report_stats.at_loss += lat.data.cpu().numpy()\r\n","            report_stats.vat_loss += lvat.data.cpu().numpy()\r\n","            report_stats.ae_loss += lae.data.cpu().numpy()\r\n","            report_stats.entropy_loss += lentropy.data.cpu().numpy()\r\n","            report_stats.n_sent += len(pred)\r\n","            report_stats.n_correct += accuracy\r\n","            self.enc_clf_opt.zero_grad()\r\n","            ltotal.backward()\r\n","\r\n","            params_list = self._get_trainabe_modules()\r\n","            if not self.config.normalize_embedding:\r\n","                params_list += list(self.embedder.parameters())\r\n","\r\n","            norm = torch.nn.utils.clip_grad_norm(params_list,\r\n","                                                 self.config.max_norm)\r\n","            report_stats.grad_norm += norm\r\n","            self.enc_clf_opt.step()\r\n","            if self.config.scheduler == \"ExponentialLR\":\r\n","                self.scheduler.step()\r\n","            self.ema_embedder.apply(self.embedder.named_parameters())\r\n","            self.ema_encoder.apply(self.encoder.named_parameters())\r\n","            self.ema_clf.apply(self.clf.named_parameters())\r\n","\r\n","            report_func(self.epoch,\r\n","                        batch_index,\r\n","                        iter_per_epoch,\r\n","                        self.time_s,\r\n","                        report_stats,\r\n","                        self.config.report_every,\r\n","                        self.logger)\r\n","\r\n","            if self.global_steps % self.config.eval_steps == 0:\r\n","                cm_, accuracy, prc_dev = self._run_evaluate(dev_data)\r\n","                self.logger.info(\"- dev accuracy {} | best dev accuracy {} \".format(accuracy, self.best_accuracy))\r\n","                self.writer.add_scalar(\"Dev_Accuracy\", accuracy,\r\n","                                       self.global_steps)\r\n","                pred_, lab_ = zip(*prc_dev)\r\n","                pred_ = torch.cat(pred_)\r\n","                lab_ = torch.cat(lab_)\r\n","                self.writer.add_pr_curve(\"Dev PR-Curve\", lab_,\r\n","                                         pred_,\r\n","                                         self.global_steps)\r\n","                pprint.pprint(cm_)\r\n","                pprint.pprint(cm_.get_all_metrics())\r\n","                if accuracy > self.best_accuracy:\r\n","                    self.logger.info(\"- new best score!\")\r\n","                    self.best_accuracy = accuracy\r\n","                    self._save_model()\r\n","                if self.config.scheduler == \"ReduceLROnPlateau\":\r\n","                    self.scheduler.step(accuracy)\r\n","                self.encoder.train()\r\n","                self.embedder.train()\r\n","                self.clf.train()\r\n","\r\n","                if self.config.weight_decay > 0:\r\n","                    print(\">> Square Norm: %1.4f \" % self._get_l2_norm_loss())\r\n","\r\n","        cm, train_accuracy, _ = self._run_evaluate(train_data)\r\n","        self.logger.info(\"- Train accuracy  {}\".format(train_accuracy))\r\n","        pprint.pprint(cm.get_all_metrics())\r\n","\r\n","        cm, dev_accuracy, _ = self._run_evaluate(dev_data)\r\n","        self.logger.info(\"- Dev accuracy  {} | best dev accuracy {}\".format(dev_accuracy, self.best_accuracy))\r\n","        pprint.pprint(cm.get_all_metrics())\r\n","        self.writer.add_scalars(\"Overall_Accuracy\",\r\n","                                {\"Train_Accuracy\": train_accuracy,\r\n","                                 \"Dev_Accuracy\": dev_accuracy},\r\n","                                self.global_steps)\r\n","        return dev_accuracy\r\n","\r\n","    @staticmethod\r\n","    def get_accuracy(cm, output, target):\r\n","        batch_size = output.size(0)\r\n","        predictions = output.max(-1)[1].type_as(target)\r\n","        correct = predictions.eq(target)\r\n","        correct = correct.float()\r\n","        if not hasattr(correct, 'sum'):\r\n","            correct = correct.cpu()\r\n","        correct = correct.sum()\r\n","        cm.add_batch(target.cpu().numpy(), predictions.cpu().numpy())\r\n","        return correct\r\n","\r\n","    def _predict_batch(self, cm, batch):\r\n","        self.embedder.eval()\r\n","        self.encoder.eval()\r\n","        self.clf.eval()\r\n","        pred = self.clf(self.encoder(self.embedder(batch),\r\n","                                     batch)[0])\r\n","        accuracy = self.get_accuracy(cm, pred.data, batch.labels.data)\r\n","        return pred, accuracy\r\n","\r\n","    def chunks(self, l, n=15):\r\n","        for i in range(0, len(l), n):\r\n","            yield l[i:i + n]\r\n","\r\n","    def _run_evaluate(self, test_data):\r\n","        pr_curve_data = []\r\n","        cm = ConfusionMatrix(self.classes)\r\n","        accuracy_list = []\r\n","        test_iter = self.chunks(test_data)\r\n","        for test_batch in test_iter:\r\n","            test_batch = seq_pad_concat(test_batch, -1)\r\n","            pred, acc = self._predict_batch(cm, test_batch)\r\n","            accuracy_list.append(acc)\r\n","            pr_curve_data.append(\r\n","                (F.softmax(pred, -1)[:, 1].data, test_batch.labels.data))\r\n","        accuracy = 100 * (sum(accuracy_list) / len(test_data))\r\n","        return cm, accuracy, pr_curve_data\r\n","\r\n","    def _train(self, train_data, dev_data, unlabel_data):\r\n","        nepoch_no_imprv = 0\r\n","\r\n","        epoch_start = self.epoch + 1\r\n","        epoch_end = self.epoch + self.config.nepochs + 1\r\n","        for self.epoch in range(epoch_start, epoch_end):\r\n","            self.logger.info(\r\n","                \"Epoch {:} out of {:}\".format(self.epoch, self.config.nepochs))\r\n","            random.shuffle(train_data)\r\n","            random.shuffle(unlabel_data)\r\n","            accuracy = self._run_epoch(train_data, dev_data, unlabel_data)\r\n","\r\n","            if accuracy > self.best_accuracy:\r\n","                nepoch_no_imprv = 0\r\n","                self.best_accuracy = accuracy\r\n","                self.logger.info(\"- new best score!\")\r\n","                self._save_model()\r\n","            else:\r\n","                nepoch_no_imprv += 1\r\n","                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\r\n","                    self.logger.info(\r\n","                        \"- early stopping {} epochs without improvement\".format(\r\n","                            nepoch_no_imprv))\r\n","                    break\r\n","            if self.config.scheduler == \"ReduceLROnPlateau\":\r\n","                self.scheduler.step(accuracy)\r\n","\r\n","    def _evaluate(self, test_data):\r\n","        self.logger.info(\"Evaluating model over test set\")\r\n","        self._load_model()\r\n","        _, accuracy, prc_test = self._run_evaluate(test_data)\r\n","        pred_, lab_ = zip(*prc_test)\r\n","        pred_ = torch.cat(pred_).cpu().tolist()\r\n","        lab_ = torch.cat(lab_).cpu().tolist()\r\n","        path_ = os.path.join(self.config.output_path, \"{}_pred_gt.tsv\".format(self.config.exp_name))\r\n","        with open(path_, 'w') as fp:\r\n","            for p, l in zip(pred_, lab_):\r\n","                fp.write(str(p) + '\\t' + str(l) + '\\n')\r\n","        self.logger.info(\"- test accuracy {}\".format(accuracy))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejyjCC16Tt6J"},"source":["class GlobalAttention(nn.Module):\r\n","    def __init__(self, dim, attn_type=\"dot\"):\r\n","        super(GlobalAttention, self).__init__()\r\n","\r\n","        self.dim = dim\r\n","        self.attn_type = attn_type\r\n","        assert (self.attn_type in [\"dot\", \"general\", \"mlp\"]), (\r\n","                \"Please select a valid attention type.\")\r\n","\r\n","        if self.attn_type == \"general\":\r\n","            self.linear_in = nn.Linear(dim, dim, bias=False)\r\n","        elif self.attn_type == \"mlp\":\r\n","            self.linear_context = nn.Linear(dim, dim, bias=False)\r\n","            self.linear_query = nn.Linear(dim, dim, bias=True)\r\n","            self.v = nn.Linear(dim, 1, bias=False)\r\n","        out_bias = self.attn_type == \"mlp\"\r\n","        self.linear_out = nn.Linear(dim*2, dim, bias=out_bias)\r\n","\r\n","        self.sm = nn.Softmax()\r\n","        self.tanh = nn.Tanh()\r\n","\r\n","    def score(self, h_t, h_s):\r\n","        src_batch, src_len, src_dim = h_s.size()\r\n","        tgt_batch, tgt_len, tgt_dim = h_t.size()\r\n","        aeq(src_batch, tgt_batch)\r\n","        aeq(src_dim, tgt_dim)\r\n","        aeq(self.dim, src_dim)\r\n","\r\n","        if self.attn_type in [\"general\", \"dot\"]:\r\n","            if self.attn_type == \"general\":\r\n","                h_t_ = h_t.view(tgt_batch*tgt_len, tgt_dim)\r\n","                h_t_ = self.linear_in(h_t_)\r\n","                h_t = h_t_.view(tgt_batch, tgt_len, tgt_dim)\r\n","            h_s_ = h_s.transpose(1, 2)\r\n","            return torch.bmm(h_t, h_s_)\r\n","        else:\r\n","            dim = self.dim\r\n","            wq = self.linear_query(h_t.view(-1, dim))\r\n","            wq = wq.view(tgt_batch, tgt_len, 1, dim)\r\n","            wq = wq.expand(tgt_batch, tgt_len, src_len, dim)\r\n","\r\n","            uh = self.linear_context(h_s.contiguous().view(-1, dim))\r\n","            uh = uh.view(src_batch, 1, src_len, dim)\r\n","            uh = uh.expand(src_batch, tgt_len, src_len, dim)\r\n","\r\n","            wquh = self.tanh(wq + uh)\r\n","\r\n","            return self.v(wquh.view(-1, dim)).view(tgt_batch, tgt_len, src_len)\r\n","\r\n","    def forward(self, input, memory_bank, memory_lengths=None):\r\n","        if input.dim() == 2:\r\n","            one_step = True\r\n","            input = input.unsqueeze(1)\r\n","        else:\r\n","            one_step = False\r\n","\r\n","        batch, sourceL, dim = memory_bank.size()\r\n","        batch_, targetL, dim_ = input.size()\r\n","        aeq(batch, batch_)\r\n","        aeq(dim, dim_)\r\n","        aeq(self.dim, dim)\r\n","        \r\n","        align = self.score(input, memory_bank)\r\n","\r\n","        if memory_lengths is not None:\r\n","            mask = sequence_mask(memory_lengths)\r\n","            mask = mask.unsqueeze(1)  \r\n","            align.data.masked_fill_(~mask, -float('inf'))\r\n","\r\n","        align_vectors = self.sm(align.view(batch*targetL, sourceL))\r\n","        align_vectors = align_vectors.view(batch, targetL, sourceL)\r\n","\r\n","        c = torch.bmm(align_vectors, memory_bank)\r\n","\r\n","        concat_c = torch.cat([c, input], 2).view(batch*targetL, dim*2)\r\n","        attn_h = self.linear_out(concat_c).view(batch, targetL, dim)\r\n","        if self.attn_type in [\"general\", \"dot\"]:\r\n","            attn_h = self.tanh(attn_h)\r\n","\r\n","        if one_step:\r\n","            attn_h = attn_h.squeeze(1)\r\n","            align_vectors = align_vectors.squeeze(1)\r\n","\r\n","            batch_, dim_ = attn_h.size()\r\n","            aeq(batch, batch_)\r\n","            aeq(dim, dim_)\r\n","            batch_, sourceL_ = align_vectors.size()\r\n","            aeq(batch, batch_)\r\n","            aeq(sourceL, sourceL_)\r\n","        else:\r\n","            attn_h = attn_h.transpose(0, 1).contiguous()\r\n","            align_vectors = align_vectors.transpose(0, 1).contiguous()\r\n","\r\n","            targetL_, batch_, dim_ = attn_h.size()\r\n","            aeq(targetL, targetL_)\r\n","            aeq(batch, batch_)\r\n","            aeq(dim, dim_)\r\n","            targetL_, batch_, sourceL_ = align_vectors.size()\r\n","            aeq(targetL, targetL_)\r\n","            aeq(batch, batch_)\r\n","            aeq(sourceL, sourceL_)\r\n","\r\n","        return attn_h, align_vectors\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNmrfH9-DlN1"},"source":["#Training"]},{"cell_type":"code","metadata":{"id":"7KoM5DwwtY--"},"source":["class hyperparameter():\r\n","    model='WordLstm'\r\n","    d_units=300\r\n","    d_proj=200\r\n","    d_hidden=512\r\n","    projection=False\r\n","    d_down_proj=100\r\n","    down_projection=False\r\n","    num_discriminator_layers=3\r\n","    frnn=False\r\n","    brnn=True\r\n","    timedistributed=False\r\n","    init_scalar=0.05\r\n","    num_layers=1\r\n","    unif=0.25\r\n","    multi_gpu=False\r\n","    gpu_ids=[0, 1, 2, 3]\r\n","    gradient_clipping=True\r\n","    max_norm=1.0\r\n","    weight_decay=0.0\r\n","    load_checkpoint=False\r\n","    use_pretrained_embeddings=True\r\n","    train_embeddings=True\r\n","    finetune=False\r\n","    home = os.environ['HOME']\r\n","    max_iter=None\r\n","    nepochs=50\r\n","    dropout=0.5\r\n","    word_dropout=0.5\r\n","    lstm_dropout=0.5\r\n","    locked_dropout=0.5\r\n","    batch_size=64\r\n","    nepoch_no_imprv=3\r\n","    nchkp_no_imprv=30\r\n","    hidden_size=1024\r\n","    subsampling=1e-4\r\n","    class_weight='uniform'\r\n","    optim='adam'\r\n","    eval_steps=1000\r\n","    lr=0.001\r\n","    lr_decay=0.5\r\n","    beta1=0.9\r\n","    beta2=0.999\r\n","    eps=1e-8\r\n","    patience=20\r\n","    scheduler='ReduceLROnPlateau'\r\n","    gamma=0.99995\r\n","    adaptive_dropout = False\r\n","    pool_type='max_pool'\r\n","    dynamic_pool_size=20\r\n","    wbatchsize=3000\r\n","    wbatchsize_unlabel=12000\r\n","\r\n","    lambda_clf=1.0\r\n","    lambda_ae=0.0\r\n","    lambda_at=1.0\r\n","    lambda_vat=1.0\r\n","    lambda_entropy=1.0\r\n","    inc_unlabeled_loss=True\r\n","    unlabeled_loss_type='AvgTrainUnlabel'\r\n","    perturb_norm_length=5.0\r\n","    max_embedding_norm=None\r\n","    normalize_embedding=False\r\n","    add_noise=False\r\n","    noise_dropout=0.1\r\n","    random_permutation=3\r\n","    debug=False\r\n","    report_every=100\r\n","    input='temp'\r\n","    save_data='demo'\r\n","    output_path=\"results/clf/\"\r\n","    exp_name='ssl'\r\n","    corpus='sst'\r\n","    model_file = exp_name + \".pt\"\r\n","    now = datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M-%S\")\r\n","    random_num = random.randint(1, 1000)\r\n","    log_path = os.path.join(output_path, \"log_{}_time-{}_rand_{}.txt\".format(now,exp_name,random_num))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUp0E8FsJXvt"},"source":["args = Args()\r\n","if not os.path.exists(args.output_path):\r\n","    os.makedirs(args.output_path)\r\n","logger = get_logger(args.log_path)\r\n","logger.info(json.dumps(args.__dict__, indent=4))\r\n","\r\n","id2label = {0:0,1:1}\r\n","# id2label = {0:1,1:2,2:3,3:4}\r\n","args.id2w = id2w\r\n","args.n_vocab = len(id2w)\r\n","args.id2label = id2label\r\n","args.num_classes = len(id2label)\r\n","\r\n","object = Training(args, logger)\r\n","object(train_data, dev_data, test_data, unlabel_data)"],"execution_count":null,"outputs":[]}]}